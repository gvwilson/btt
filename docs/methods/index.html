<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Research Methods</title>
<link href="../static/favicon.ico" rel="icon" type="image/x-icon"/>
<link href="../static/page.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<nav>
<a href="../">Home</a>
      ·
      <span class="dropdown">
<a href="#">Lessons</a>
<span class="dropdown-content" id="nav-lessons">
<a href="../intro/">Introduction</a>
<a href="../important/">The Important Stuff</a>
<a href="../starting/">Starting</a>
<a href="../teams/">Teams</a>
<a href="../rules-persuade/">How to Talk People Into Things</a>
<a href="../conflict/">Managing Conflict</a>
<a href="../git-solo/">Using Git On Your Own</a>
<a href="../git-team/">Using Git Together</a>
<a href="../ip/">Intellectual Property</a>
<a href="../communicate/">Communicating</a>
<a href="../testing/">Testing</a>
<a href="../design/">Software Design</a>
<a href="../security/">Security</a>
<a href="../errors/">Error Handling</a>
<a href="../debugging/">Debugging</a>
<a href="../automation/">Automation</a>
<a href="../tooling/">Tooling</a>
<a href="../rules-comfortable/">How to Make Yourself Comfortable</a>
<a href="../process/">Process</a>
<a href="../rules-joining/">How to Join an Existing Project</a>
<a href="../rules-newcomers/">How to Welcome Newcomers</a>
<a href="../research/">Research</a>
<a href="../rules-research/">How to be a Good Research Partner</a>
<a href="../fairness/">Fair Play</a>
<a href="../rules-fired/">How to Handle Being Fired</a>
<a href="../delivery/">Wrapping Up</a>
<a href="../rules-handover/">How to Hand Over and Move On</a>
<a href="../finale/">Conclusion</a>
</span>
</span>
      ·
      <span class="dropdown">
<a href="#">Extras</a>
<span class="dropdown-content" id="nav-extras">
<a href="../license/">License</a>
<a href="../conduct/">Code of Conduct</a>
<a href="../contrib/">Contributing</a>
<a href="../bibliography/">Bibliography</a>
<a href="../glossary/">Glossary</a>
<a href="../thinking/">Thinking</a>
<a href="../methods/">Research Methods</a>
<a href="../onboarding/">Onboarding</a>
<a href="../project-eval/">Project Evaluation</a>
<a href="../personal-eval/">Personal Evaluation</a>
<a href="../reading/">Further Reading</a>
<a href="../rules-freelance/">How to Get Started Freelancing</a>
<a href="../rules-change/">How to Change the World</a>
<a href="../authors/">Authors</a>
</span>
</span>
</nav>
<main>
<h1>Research Methods</h1>
<p class="subtitle">where does our understanding come from</p>
<p>Back in 2016 [I tweeted][excel-tweet], "If anyone has data showing [% i "Excel" %]Excel[% /i %] is <em>more</em> error-prone than [% i "MATLAB" %]MATLAB[% /i %]/[% i "Python" %]Python[% /i %]/[% i "R (programming
language)" %]R[% /i %] once you normalize for hours spent learning it, please
post."  It was clear from the responses and from other discussion online that
most programmers believe this, but I'm not really sure what "this" is:</p>
<dl>
<dt>There are more errors in published results created with Excel than in results created with scripting languages like MATLAB, Python, and R.</dt>
<dd>Given that many more people use Excel, that's like saying that people in
China have more heart attacks than people in Luxembourg.</dd>
<dt>Results calculated with Excel are more likely to be wrong than results calculated with scripting languages.</dt>
<dd>
<p>This is what I had in mind when I tweeted, and I don't think the answer is
obvious.  Yes, there are lots of examples of people botching spreadsheets,
but there's also a lot of buggy code out there.  ([% i "Flon's
Axiom" %]Flon's Axiom[% /i %] states, "There is not now, nor has there ever
been, nor will there ever be, any programming language in which it is the
least bit difficult to write bad code.")</p>
<p>And even if this claim is true, correlation isn't causation.  I think that
people who do statistics programmatically have probably invested more time
in mastering their tools than people who use spreadsheets.  Any differences
in error rates could easily be due to differences in time spent in
reflective practice.</p>
</dd>
<dt>People who are equally proficient in Excel and scripting languages are more likely to make mistakes in Excel.</dt>
<dd>This formulation corrects the flaw identified above, but is nonsensical,
since the only meaningful definition of "equally proficient" is "use equally
well".</dd>
<dt>Spreadsheets are intrinsically more error-prone than scripting languages</dt>
<dd>I have heard people claim that spreadsheets don't show errors as clearly,
they're harder to test, it's harder to figure out what calculations are
actually being done, or they themselves are buggier than scripting
languages' math libraries.  These are all plausible, but may all be red
herrings.  Yes, it's hard to write unit tests for spreadsheets, but it's
possible: [% b Hermans2016 %] found that 8% of spreadsheets included
tests like <code>if(A1&lt;&gt;5, "ERROR", "OK")</code>.  I'd be surprised if more than 8% of
people who do statistics in Python or R regularly write unit tests for their
scripts, so the fact that they <em>could</em> is irrelevant.</dd>
</dl>
<p>To be clear, I'm not defending or recommending spreadsheets.  But if programming
really is a better way to do crunch numbers, we ought to be able to use science
to prove it.  And to do that, we have to know what methods we have at our
disposal and what they're good for.</p>
<h2>Controlled Experiments</h2>
<p>The first method that empirical research uses is [% i "controlled
experiments" %]controlled experiments[% /i %]: subjects are given a task and some
aspect of their performance is measured.  Subjects are typically divided into a
[% i "control group" %][% g control_group %]control group[% /g %][% /i %] (who do things as
normal) and a [% i "treatment group" %][% g treatment_group %]treatment group[% /g %][% /i %] (who do things in an alternative way). If the difference between
the groups is large enough statistically, the experimenter can say that the
treatment probably has an effect on outcomes.</p>
<p>There are several traps in this for the unwary [% b DeOliveiraNeto2019 %]:</p>
<dl>
<dt>[% i "experimenter bias" %]Experimenter bias[% /i %].</dt>
<dd>People have many biases, both conscious and unconscious.  In order to make
sure that these don't inadvertently affect the results, subjects should be
assigned to groups at random. Going even further, experiments in medicine
are often [% i "double blind experiments" %][% g double_blind %]double blind[% /g %][% /i %]: neither the subject nor the person administering the treatment
knows which subjects are getting the new heart medication and which are
getting a [% i "placebo" %][% g placebo %]placebo[% /g %][% /i %]. It's usually not
possible to achieve this when doing software engineering experiments.</dd>
<dt>[% i "significance hacking" %]Significance hacking[% /i %].</dt>
<dd>If you measure enough things and look for enough correlations, you will
almost certainly find <em>something</em> that passes a test for statistical
significance. For example, there is a strong correlation between the number
of letters in winning words in spelling competitions and the number of
people killed by venomous spiders [% b Vigen2015 %]. To guard against
this, researchers should [% i "pre-registration of experiments" %][% g pre_registration %]pre-register[% /g %][% /i %] their analyses, i.e., say in advance what
they're going to compare against what, and then use various statistical
techniques that require a higher standard of proof when they are checking
more possible combinations.</dd>
<dt>[% i "negative results (failure to publish)" %]Failure to publish negative results[% /i %].</dt>
<dd>An experiment isn't a failure if it doesn't find something that is
statistically significant: ruling something out is just as useful as finding
something new. However, negative results are not as exciting (and not as
beneficial to a researcher's career), so they often go unreported.</dd>
</dl>
<p>Getting a few undergraduates in a lab for an hour each is easy; getting
professional programmers to do something new or different for several months is
not, but this doesn't mean that controlled experiments have to be expensive.  If
you have an idea you want to test, gather a little data and use that to evaluate
the plausibility of your idea. If you need to be really confident, build up the
evidence base over time in multiple experiments, increasing the sample size and
improving the methodology each time to get a stronger answer for a more specific
question.</p>
<p>Researchers often rely on [% i "quasi-experiment" %][% g quasi_experiment %]quasi-experiments[% /g %][% /i %] when doing this, i.e., they look
at pre-existing groups like programmers who have decided for themselves to use a
new IDE against ones who have not. Quasi-experiments are cheaper and easier to
set up, but researchers must be careful to account for [% i "confounding variable" %][% g confounding_variables %]confounding variables[% /g %][% /i %]. For example, are programmers who choose to use an IDE younger
and therefore less experienced than ones who use legacy text editors? If so, how
does that difference skew the results?</p>
<h2>Data Mining</h2>
<p>Quasi-experiments blend into the second major research approach, which uses
statistics and machine learning to find patterns in whatever data the researcher
can get her hands on. Doing this is called [% i "data mining" %][% g data_mining %]data mining[% /g %][% /i %], and most studies of this kind make use of the wealth
of information available online at sites like [GitHub]][github] and [Stack
Overflow][stack-overflow] or from millions of crash reports collected online
[% b Glerum2009 %].  Data mining has produced many valuable insights, but
has challenges of its own.  The largest of these is that people who work in the
open aren't typical, so any results we get from studying them must be
interpreted cautiously.</p>
<p>One problem with data mining is that once again, correlation doesn't imply
causation.  Another is that we usually don't know whether the data we have
access to is representative.  In 2012, Scott Hanselman coined the term [% i "dark matter developer" %][% g dark_matter_developer %]dark matter developers[% /g %][% /i %] to describe programmers who don't blog, don't answer questions
in online forums, don't have their work in public repositories, and so on. Just
as dark matter makes up most of the universe, dark matter developers make up
most of our industry, and just as the matter we can see is vanishingly atypical,
so too are developers who radiate information.</p>
<p>One reason for this is that the web ranges from unwelcoming to actively hostile
for people from under-represented groups [% b Ford2016 May2019 %].
Unfortunately, in-person workplaces are often no better: many are filled with
small signs that make many people feel out of place [% b Cheryan2009 %].
[% x fairness %] takes a closer look at these issues.</p>
<blockquote>
<h3>A cautionary tale</h3>
<p>[% b Zeller2011 %] did what too many researchers in too many fields do on a
regular basis: throw some data at some machine learning algorithms and then
claim that whatever comes out the other end is significant. Luckily for us, they
did it on purpose to make a point.</p>
<p>They took data on code and errors for the Eclipse IDE and correlated the two to
find good predictors of bugs. Which sounds sensible—except they did the
correlation at the level of individual characters. It turns out that 'i', 'r',
'o', and 'p' are most strongly correlated with errors. What is a sensible
researcher to do facing these findings? Take those letters out of the keyboard,
of course.  The authors then go over everything that's wrong with their
approach, from lack of theoretical grounding to dishonest use of
statistics. Before you read too much research, make sure to read this.</p>
</blockquote>
<h2>Qualitative Methods</h2>
<p>The third set of approaches are called [% i "qualitative methods" %][% g qualitative_method %]qualitative methods[% /g %][% /i %], and involve close analysis
of a small number of cases to tease out common patterns.  Articles like
[% b Sharp2016 %] do an excellent job of explaining how these methods work
and what their strengths and limitations are.</p>
<blockquote>
<h3>Wish I knew then what I know now</h3>
<p>My classes in engineering taught me to look down on anything that wasn't a
controlled laboratory experiment whose results could be neatly displayed in a
scatterplot or bar chart.  It wasn't until I was in my thirties that I accepted
that the "fuzzy" methods of the social sciences were just as rigorous when used
properly, and the only ones that could produce certain valuable insights.</p>
</blockquote>
<p>[% b Washburn2016 %] demonstrates the kinds of insights these methods can
produce. They analyzed 155 postmortem reviews of game projects to identify
characteristics of game development, link the characteristics to positive and
negative experiences, and distill a set of best practices and pitfalls for game
development. Their description of their method is worth repeating in full:</p>
<blockquote>
<p>Initially, we started with 12 categories of common aspects of development…  In
order to identify additional categories, we performed 3 iterations of analysis
and identification.  The first week, we each read and analyzed 3 postmortem
reviews each, classifying the items discussed in each section into the 12
predetermined categories of common aspects that impact development.  While
analyzing these reviews, we identified additional categories of items that went
right or wrong during development, and revisited the reviews we had already
analyzed to update the categorization of items. For the next two weeks we
repeated this process of analyzing postmortems and identifying categories,
analyzing 10 postmortems each in week 2, and 15 postmortems each in week
3. After each iteration, we discussed the additional categories we identified,
and determined if they were viable.</p>
<p>After our initial iterations for identifying additional categories, we had
completed the analysis of 60 postmortem reviews.  We then stopped identifying
new categories, and began analyzing postmortems at a combined rate of about 40
postmortem reviews per week.  After each week we reviewed what we had done to
ensure we both had the same understanding of each category.  This continued
until we had analyzed all the postmortem reviews.</p>
</blockquote>
<h2>The Fourth Tradition</h2>
<blockquote>
<p>"Once the rockets are up, who cares where they come down?
<br/>
That's not my department!" says Wernher von Braun</p>
<p>— [% i "Lehrer, Tom" %]Tom Lehrer[% /i %]</p>
</blockquote>
<p>[% b Tedre2008 %] describes three traditions that have shaped how we think
about computing: the [% i "mathematical tradition in
computing" %]mathematical[% /i %], which focuses on algorithms and proofs; the
[% i "scientific tradition in computing" %]scientific[% /i %], which studies
programs and programmers empirically; and the [% i "engineering tradition in
computing" %]engineering tradition[% /i %], which centers the fact that computing
matters because we can actually build useful things.</p>
<p>That paper changed how I think about our field, but in the past few years I have
realized that [% i "humanist tradition in computing" %]another point of
view[% /i %] is just as important, though not as well respected.  It draws on
humanities and social sciences to explore questions like, "Who does this help?",
"Who does this hurt?", and, "Who decides?"  Just as the most interesting
software engineering research these days is look at how the way we think
interacts with the way we program ([% x research %]), the most interesting
thinking about computing as a whole is coming from people who have outgrown the
"Wernher von Braun" mentality of Lehrer's song.</p>
</main>
<footer>
<a href="../">Organizational Change for Open Science</a>
      copyright © 2025
      <a href="../#acknowledgments">the authors</a>
</footer>
</body>
</html>